{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMENKVbuOfQvEjb/uP4Xuv8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagaust/DH-Computational-Methodologies/blob/main/Loading_from_Colab_to_BigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO4uYnT-fWj2"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --upgrade google-cloud-bigquery\n",
        "!pip install pandas\n",
        "\n",
        "# Import libraries\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate to BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Initialize BigQuery client\n",
        "project_id = 'philedu-416212'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Define your dataset and table names\n",
        "dataset_id = 'philCurriculum'\n",
        "table_name = 'your_table_name'\n",
        "\n",
        "# Read CSV files into pandas DataFrame\n",
        "# Assuming you have CSV files named file1.csv, file2.csv, etc.\n",
        "file_paths = ['file1.csv', 'file2.csv']  # Add more file paths if needed\n",
        "dataframes = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate DataFrames if needed\n",
        "concatenated_df = pd.concat(dataframes)\n",
        "\n",
        "# Define BigQuery schema (you need to define it according to your CSV file structure)\n",
        "schema = [\n",
        "    bigquery.SchemaField(name, 'STRING') for name in concatenated_df.columns\n",
        "]\n",
        "\n",
        "# Upload DataFrame to BigQuery table\n",
        "table_ref = client.dataset(dataset_id).table(table_name)\n",
        "job_config = bigquery.LoadJobConfig(schema=schema, skip_leading_rows=1)\n",
        "job = client.load_table_from_dataframe(concatenated_df, table_ref, job_config=job_config)\n",
        "\n",
        "# Wait for the job to complete\n",
        "job.result()\n",
        "\n",
        "print(f'CSV files uploaded to BigQuery table {dataset_id}.{table_name} successfully.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUMnUgjBhdmW",
        "outputId": "9bccd642-59f3-43c5-b5e1-760f3328067b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for as\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the os module\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Get list of CSV files in the default directory\n",
        "file_paths = [f for f in os.listdir() if f.endswith('.csv') and os.path.isfile(f)]\n",
        "\n",
        "# Iterate through CSV files and attempt to load each one\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        pd.read_csv(file_path, nrows=1)  # Attempt to read the first row to infer schema\n",
        "    except Exception as e:\n",
        "        print(f'Error reading file: {file_path}')\n",
        "        print(f'Error message: {str(e)}')\n"
      ],
      "metadata": {
        "id": "AuiaRnxlg7K-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core.exceptions import NotFound\n",
        "\n",
        "# Define the table name and schema\n",
        "table_name = 'your_table_name'\n",
        "schema = [\n",
        "    bigquery.SchemaField('column1', 'STRING'),\n",
        "    bigquery.SchemaField('column2', 'INTEGER'),\n",
        "    # Add more fields as needed based on your data\n",
        "]\n",
        "\n",
        "# Create table reference\n",
        "table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "# Check if table exists and delete it if it does\n",
        "try:\n",
        "    client.get_table(table_ref)\n",
        "    client.delete_table(table_ref)\n",
        "    print(f'Table {table_name} already exists and has been deleted.')\n",
        "except NotFound:\n",
        "    pass\n",
        "\n",
        "# Create the table\n",
        "table = bigquery.Table(table_ref, schema=schema)\n",
        "table = client.create_table(table)\n",
        "\n",
        "# Load data into the table\n",
        "job_config = bigquery.LoadJobConfig(schema=schema, skip_leading_rows=1, max_bad_records=10)  # Adjust as needed\n",
        "job = client.load_table_from_dataframe(concatenated_df, table_ref, job_config=job_config)\n",
        "\n",
        "# Wait for the job to complete\n",
        "job.result()\n",
        "\n",
        "print(f'Table {table_name} created and data loaded successfully.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "R1g4w4D3gALD",
        "outputId": "190369b2-8c04-403b-e37d-ec536cbd37c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "bq_schema contains fields not present in dataframe: {'column2', 'column1'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a2883e4eaff0>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Load data into the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mjob_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadJobConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_leading_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bad_records\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_table_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Wait for the job to complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mload_table_from_dataframe\u001b[0;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[1;32m   2678\u001b[0m                 ]\n\u001b[1;32m   2679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2680\u001b[0;31m         new_job_config.schema = _pandas_helpers.dataframe_to_bq_schema(\n\u001b[0m\u001b[1;32m   2681\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_job_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2682\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdataframe_to_bq_schema\u001b[0;34m(dataframe, bq_schema)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# column, but it was not found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbq_schema_unused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    448\u001b[0m             \"bq_schema contains fields not present in dataframe: {}\".format(\n\u001b[1;32m    449\u001b[0m                 \u001b[0mbq_schema_unused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bq_schema contains fields not present in dataframe: {'column2', 'column1'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhsa6Ejyk5bz",
        "outputId": "bcfc3236-c9ef-40e4-a54e-7f13b170da6d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.17.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (23.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.31.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (3.20.3)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.colab import auth\n",
        "import os\n",
        "\n",
        "# Authenticate and create a client\n",
        "auth.authenticate_user()\n",
        "project_id = 'philedu-416212'  # Replace with your project ID\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Define your BigQuery Dataset\n",
        "dataset_id = 'philCurriculum'  # Replace with your dataset ID\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "\n",
        "# Directory where your CSV files are stored\n",
        "directory = \"/content/\"  # Assuming all CSVs are in the '/content/' directory\n",
        "\n",
        "# Iterate over each file in the directory and load it into BigQuery\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        table_id = filename.split('.')[0]  # Assuming table name is same as filename\n",
        "        table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "        job_config = bigquery.LoadJobConfig()\n",
        "        job_config.source_format = bigquery.SourceFormat.CSV\n",
        "        job_config.skip_leading_rows = 1  # Set to 0 if no header row\n",
        "        job_config.autodetect = True\n",
        "        job_config.max_bad_records = 10  # Allows up to 10 bad records before failing\n",
        "\n",
        "\n",
        "        with open(os.path.join(directory, filename), \"rb\") as source_file:\n",
        "            job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
        "\n",
        "        job.result()  # Waits for the job to complete\n",
        "\n",
        "        print(f\"Loaded {filename} into {dataset_id}.{table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "z-fmmOBQlAtt",
        "outputId": "02484394-06b0-466c-c08b-5079bdbd6baa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequest",
          "evalue": "400 Provided Schema does not match Table philedu-416212:philCurriculum.C972EE73-8DDF-E520-B940-E354349AB9F0. Cannot add fields (field: Date_Added); reason: invalid, message: Provided Schema does not match Table philedu-416212:philCurriculum.C972EE73-8DDF-E520-B940-E354349AB9F0. Cannot add fields (field: Date_Added); reason: invalid, message: It looks like you are appending to an existing table with autodetect enabled. Disabling autodetect may resolve this.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-293946420031>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_table_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Waits for the job to complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {filename} into {dataset_id}.{table_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 Provided Schema does not match Table philedu-416212:philCurriculum.C972EE73-8DDF-E520-B940-E354349AB9F0. Cannot add fields (field: Date_Added); reason: invalid, message: Provided Schema does not match Table philedu-416212:philCurriculum.C972EE73-8DDF-E520-B940-E354349AB9F0. Cannot add fields (field: Date_Added); reason: invalid, message: It looks like you are appending to an existing table with autodetect enabled. Disabling autodetect may resolve this."
          ]
        }
      ]
    }
  ]
}