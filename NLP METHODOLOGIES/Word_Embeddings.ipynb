{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0un40KdV+Sxm8iUpR3K+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagaust/DH-Computational-Methodologies/blob/main/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings and Semantic Similarity\n",
        "\n",
        "---\n",
        "\n",
        "**Definition:**  \n",
        "Word Embeddings are numerical representations of words, usually as vectors in a high-dimensional space. The idea is that words with similar meanings or usages will have similar vectors, i.e., they will be close in this vector space. This property allows us to measure semantic similarity between words.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Why are Word Embeddings Important?**\n",
        "\n",
        "1. **Semantic Meaning**: They capture the semantic meaning of words, which is not possible with simpler representations like one-hot encoding.\n",
        "2. **Dimensionality Reduction**: Represent words in a more compact form compared to sparse representations.\n",
        "3. **Versatility**: Useful in a wide range of NLP tasks including text classification, sentiment analysis, and machine translation.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† **How Do Word Embeddings Work?**\n",
        "\n",
        "Word embeddings are trained on large corpora to capture semantic relationships between words. They use the context in which words appear to determine word similarities. The idea is that words appearing in similar contexts tend to have similar meanings.\n",
        "\n",
        "---\n",
        "\n",
        "## üåê **Popular Word Embedding Models**:\n",
        "\n",
        "- **Word2Vec**: Developed by Google, it offers two training algorithms: Continuous Bag of Words (CBOW) and Skip-Gram.\n",
        "- **GloVe (Global Vectors for Word Representation)**: Developed by Stanford, it's based on factorizing the word co-occurrence matrix.\n",
        "- **FastText**: Developed by Facebook, it's an extension of Word2Vec. Unlike Word2Vec, which treats each word in the corpus as an atomic entity, FastText represents a word as a bag of character n-grams.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö **Applications of Word Embeddings**:\n",
        "\n",
        "1. **Text Classification**: Improve accuracy by using semantically rich word representations.\n",
        "2. **Information Retrieval**: Search for documents that are semantically related to a query.\n",
        "3. **Sentiment Analysis**: Understand the sentiment of texts by capturing the semantic meaning of the words.\n",
        "4. **Machine Translation**: Translate between languages by mapping words to a common semantic space.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° **Insights from Word Embeddings**:\n",
        "\n",
        "1. **Semantic Relationships**: Discover relationships like \"man\" is to \"woman\" as \"king\" is to \"queen\".\n",
        "2. **Topic Identification**: Identify what topics words are related to based on their embeddings.\n",
        "3. **Language Structure**: Uncover syntactic relationships between words.\n",
        "\n",
        "---\n",
        "\n",
        "## üõë **Challenges with Word Embeddings**:\n",
        "\n",
        "1. **Requires Large Data**: Reliable word embeddings typically require training on large corpora.\n",
        "2. **Static Representation**: Traditional word embeddings offer a static representation, meaning a word has the same vector regardless of context (though models like BERT have addressed this).\n",
        "3. **Storage**: High-dimensional vectors for large vocabularies can take up significant storage.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **Word Embeddings in Python**:\n",
        "\n",
        "Python libraries like Gensim provide tools to work with word embeddings. Here's a simple example using Gensim's Word2Vec:\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample data\n",
        "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"bark\"]]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Find vector for a word\n",
        "vector = model.wv['cat']\n",
        "\n",
        "# Find similar words\n",
        "similar = model.wv.most_similar('cat', topn=5)\n"
      ],
      "metadata": {
        "id": "ZqSFncjejpQI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2OtYaIpjla_"
      },
      "outputs": [],
      "source": []
    }
  ]
}