{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY4N46Yb3qp4cR+R+DfdbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagaust/DH-Computational-Methodologies/blob/main/Text_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopword Removal and Text Cleaning\n",
        "\n",
        "---\n",
        "\n",
        "**Definition:**  \n",
        "Stopword Removal is the process of eliminating common words (like \"and\", \"the\", \"in\") from text data. Text Cleaning involves refining the data by removing any noise, such as special characters, numbers, or irrelevant spaces. These processes help highlight the more meaningful content in the text, aiding various NLP tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Why is Text Cleaning Important?**\n",
        "\n",
        "1. **Enhance Signal-to-Noise Ratio**: Removing irrelevant content improves the quality of data for downstream tasks.\n",
        "2. **Efficiency**: Reduces computational overhead by limiting the size of the data.\n",
        "3. **Accuracy**: By eliminating noise, models can focus on relevant patterns, leading to better results.\n",
        "4. **Standardization**: Ensures consistency in data, which is crucial for training robust models.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† **Components of Text Cleaning**:\n",
        "\n",
        "- **Lowercasing**: Convert all characters in the text to lowercase to maintain uniformity.\n",
        "- **Punctuation Removal**: Eliminate symbols and punctuation marks.\n",
        "- **Number Removal**: Depending on the task, numbers might be irrelevant and can be removed.\n",
        "- **Whitespace Removal**: Eliminate unnecessary spaces, tabs, or newlines.\n",
        "- **HTML Tag Removal**: When scraping data from the web, it's common to encounter HTML tags that need to be stripped.\n",
        "- **Stopword Removal**: As mentioned, this involves eliminating commonly used words that don't carry significant semantic value.\n",
        "\n",
        "---\n",
        "\n",
        "## üåê **Importance of Stopword Removal**:\n",
        "\n",
        "While stopwords are essential for sentence construction, they often don't carry critical meaning on their own. In tasks like keyword extraction, topic modeling, or text classification, removing stopwords can lead to more accurate and meaningful results.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö **Applications of Text Cleaning**:\n",
        "\n",
        "1. **Text Classification**: Cleaned text ensures the classifier focuses on relevant patterns.\n",
        "2. **Information Retrieval**: Enhances search results by removing noise.\n",
        "3. **Sentiment Analysis**: Improves accuracy by focusing on meaningful content.\n",
        "4. **Topic Modeling**: Ensures topics are generated based on relevant terms and not on noise or common words.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° **Challenges with Text Cleaning**:\n",
        "\n",
        "1. **Loss of Information**: Over-cleaning can sometimes remove contextually important information.\n",
        "2. **Language Dependency**: Stopwords vary across languages; using the wrong list can be counterproductive.\n",
        "3. **Ambiguity**: Words that are stopwords in one context might be relevant in another.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **Text Cleaning in Python**:\n",
        "\n",
        "Python's Natural Language Toolkit (NLTK) offers tools for text cleaning and stopword removal:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"The sun rises in the east and sets in the west.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n",
        "\n",
        "# Remove punctuation\n",
        "cleaned_tokens = [token for token in filtered_tokens if token.isalnum()]\n",
        "\n",
        "print(cleaned_tokens)\n"
      ],
      "metadata": {
        "id": "EOHT9eF_nA-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa1Y7A2tm82W"
      },
      "outputs": [],
      "source": []
    }
  ]
}