{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL9CzgX5sgZ4jr7HYJUij7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagaust/DH-Computational-Methodologies/blob/main/Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "---\n",
        "\n",
        "**Definition:**  \n",
        "Topic Modeling is a type of statistical model used in Natural Language Processing (NLP) and text mining to discover abstract topics within a collection of documents. It helps in uncovering hidden thematic structures in a large corpus.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Why is Topic Modeling Important?**\n",
        "\n",
        "1. **Content Summarization**: Provides a high-level view of the themes in large datasets.\n",
        "2. **Content Recommendation**: Recommend articles or documents similar to a given topic.\n",
        "3. **Data Organization**: Organize content based on discovered topics, making it easier to manage and retrieve.\n",
        "4. **Insights Discovery**: Understand main themes or trends in datasets like news articles over time.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† **How Does Topic Modeling Work?**\n",
        "\n",
        "Topic modeling algorithms, like LDA, work by:\n",
        "1. **Decomposing**: Breaking down texts into individual words or tokens.\n",
        "2. **Clustering**: Grouping tokens that frequently occur together across different documents.\n",
        "3. **Assigning**: Allocating topics to documents based on token clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## üåê **Common Algorithms**:\n",
        "\n",
        "- **Latent Dirichlet Allocation (LDA)**: The most popular topic modeling technique, it assumes each document is a mix of topics and a topic is a mix of words.\n",
        "- **Non-Negative Matrix Factorization (NMF)**: Based on linear algebra, it factorizes the given document-term matrix into two lower-dimensional matrices.\n",
        "- **Latent Semantic Analysis (LSA)**: Similar to LDA but, in addition to term-document matrix factorization, it considers singular value decomposition.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö **Applications of Topic Modeling**:\n",
        "\n",
        "1. **Content Recommendation**: Suggest articles or content based on user's reading history.\n",
        "2. **Search Engines**: Enhance search results by focusing on main topics.\n",
        "3. **Content Summarization**: Provide summarized views of large volumes of text.\n",
        "4. **Market Research**: Analyze customer reviews to identify main topics of discussion.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° **Insights from Topic Modeling**:\n",
        "\n",
        "1. **Content Categorization**: Understand the variety of themes present in a corpus.\n",
        "2. **Trend Analysis**: Discover emerging topics or trends over time in datasets like news articles.\n",
        "3. **Content Gap Analysis**: Identify areas or topics not covered in a corpus, useful for content creators.\n",
        "\n",
        "---\n",
        "\n",
        "## üõë **Challenges in Topic Modeling**:\n",
        "\n",
        "1. **Number of Topics**: Deciding the number of topics the algorithm should find in the corpus can be tricky.\n",
        "2. **Interpreting Topics**: The topics generated are clusters of words, which might sometimes be hard to interpret.\n",
        "3. **Noise**: Noisy data or irrelevant words can affect the quality of topics generated.\n",
        "4. **Dynamic Content**: For continuously updating datasets, the model needs frequent retraining.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **Topic Modeling in Python**:\n",
        "\n",
        "Python libraries like Gensim provide easy-to-use implementations of LDA and other topic modeling algorithms. Here's a simple example using Gensim:\n",
        "\n",
        "```python\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Sample data\n",
        "documents = [\"This is about cars\", \"This document is about bikes\", \"Bikes and cars are popular means of transport\"]\n",
        "\n",
        "# Tokenization\n",
        "texts = [[text for text in doc.split()] for doc in documents]\n",
        "\n",
        "# Create a corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Apply LDA\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
        "topics = lda_model.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "id": "-6iANGyfhPKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7JNcp10hBry"
      },
      "outputs": [],
      "source": []
    }
  ]
}